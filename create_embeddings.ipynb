{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "from typing import List\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import fitz\n",
    "from pinecone import Pinecone\n",
    "\n",
    "from src.chat_with_jfk_files.constants import PDF_BASE_DIR\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "client = AsyncOpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 40\n",
      "Total size: 0.14 GB\n"
     ]
    }
   ],
   "source": [
    "def filter_pdfs(min_pages: int, size_cutoff_percentage: float) -> list:\n",
    "    \"\"\"\n",
    "    Filter PDFs based on the number of pages and size.\n",
    "\n",
    "    :param base_dir: The directory to filter PDFs from.\n",
    "    :param min_pages: The minimum number of pages a PDF must have to be included.\n",
    "    :return: A list of filtered PDF paths.\n",
    "    \"\"\"\n",
    "    filename_to_path = {}\n",
    "    for folder in os.listdir(PDF_BASE_DIR):\n",
    "        if folder == '.DS_Store':\n",
    "            continue\n",
    "        folder_path = os.path.join(PDF_BASE_DIR, folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(folder_path, file)\n",
    "                basename = os.path.basename(pdf_path)\n",
    "\n",
    "                # we only want unique filenames\n",
    "                if basename not in filename_to_path:\n",
    "                    filename_to_path[basename] = pdf_path\n",
    "    \n",
    "    pdf_paths = list(filename_to_path.values())\n",
    "\n",
    "    filtered_pdf_paths = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            page_count = len(doc)\n",
    "\n",
    "            if page_count >= min_pages:\n",
    "                filtered_pdf_paths.append(pdf_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {e}\")\n",
    "\n",
    "    pdf_sizes = [(pdf, os.path.getsize(pdf)) for pdf in filtered_pdf_paths]\n",
    "    pdf_sizes.sort(key=lambda x: x[1])\n",
    "    cutoff_index = int(len(pdf_sizes) * size_cutoff_percentage)\n",
    "    filtered_pdf_paths = [pdf for pdf, _ in pdf_sizes[cutoff_index:]]\n",
    "\n",
    "    return filtered_pdf_paths\n",
    "\n",
    "filtered_pdfs = filter_pdfs(min_pages=5, size_cutoff_percentage=0.2)\n",
    "total_size_bytes = sum(os.path.getsize(f) for f in filtered_pdfs)\n",
    "total_size_gb = total_size_bytes / 1024 ** 3\n",
    "\n",
    "print(f\"Total files: {len(filtered_pdfs)}\")\n",
    "print(f\"Total size: {total_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  98%|█████████▊| 39/40 [03:58<00:06,  6.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.chat_with_jfk_files.ocr import extract_text_from_pdf\n",
    "\n",
    "def process_pdfs_parallel(paths: list, output_file: str, max_workers: int = 4):\n",
    "    \"\"\"\n",
    "    Process a list of PDFs in parallel with logging, progress tracking, and incremental saving.\n",
    "    \n",
    "    :param paths: List of PDF file paths to process.\n",
    "    :param output_file: Path to the JSON file for saving results.\n",
    "    :param max_workers: Number of parallel workers.\n",
    "    \"\"\"\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_path = {executor.submit(extract_text_from_pdf, path): path for path in paths}\n",
    "        for future in tqdm(as_completed(future_to_path), total=len(paths), desc=\"Processing PDFs\"):\n",
    "            path = future_to_path[future]\n",
    "            try:\n",
    "                text = future.result()\n",
    "                parent_folder = os.path.basename(os.path.dirname(path))\n",
    "                filename = os.path.basename(path)\n",
    "                result = {\"path\": os.path.join(parent_folder, filename), \"text\": text}\n",
    "\n",
    "                # append to file\n",
    "                with open(output_file, \"a\") as f:\n",
    "                    f.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "output_jsonl_file = \"ocr_output.jsonl\"\n",
    "process_pdfs_parallel(filtered_pdfs, output_jsonl_file, max_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n"
     ]
    }
   ],
   "source": [
    "def create_chunks(documents: list, min_chunk_length: int = 2500) -> list:\n",
    "    \"\"\"\n",
    "    Create chunks of text from a list of documents\n",
    "\n",
    "    document.keys() -> ['path', 'text']\n",
    "\n",
    "    :param documents: List of documents to chunk\n",
    "    :param min_chunk_length: The minimum length of a chunk in characters\n",
    "    :return: A list of chunks\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []\n",
    "    for document in documents:\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        \n",
    "        doctext = document['text']\n",
    "        parts = doctext.split('\\n\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "            if current_chunk:\n",
    "                if len(current_chunk) + len(part) + 1 > min_chunk_length:\n",
    "                    chunks.append({\"path\": document[\"path\"], \"chunk\": current_chunk})\n",
    "                    current_chunk = part\n",
    "                else:\n",
    "                    current_chunk += \" \" + part\n",
    "            else:\n",
    "                current_chunk = part\n",
    "        if current_chunk:\n",
    "            chunks.append({\"path\": document[\"path\"], \"chunk\": current_chunk})\n",
    "    return chunks\n",
    "\n",
    "\n",
    "with open(\"ocr_output.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = [json.loads(line) for line in f]\n",
    "chunks = create_chunks(documents)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing batch 1 (0 to 99)... 297 chunks remaining out of 397.\n",
      "INFO:root:Processing batch 2 (100 to 199)... 197 chunks remaining out of 397.\n",
      "INFO:root:Processing batch 3 (200 to 299)... 97 chunks remaining out of 397.\n",
      "INFO:root:Processing batch 4 (300 to 399)... -3 chunks remaining out of 397.\n"
     ]
    }
   ],
   "source": [
    "from src.chat_with_jfk_files import llm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "async def embed_chunk(chunk: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Embed a single chunk.\n",
    "\n",
    "    :param chunk: The chunk to embed\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    response = await llm.embed(chunk[\"chunk\"])\n",
    "    return {\n",
    "        \"path\": chunk[\"path\"],\n",
    "        \"chunk\": chunk[\"chunk\"],\n",
    "        \"embedding\": response\n",
    "    }\n",
    "\n",
    "async def process_batch(batch: List[dict], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Process a batch of chunks and append embeddings to a file.\n",
    "\n",
    "    :param batch: A list of chunks to embed\n",
    "    :param file_path: The file to append results to\n",
    "    \"\"\"\n",
    "    embeddings = await asyncio.gather(*(embed_chunk(chunk) for chunk in batch))\n",
    "    with open(file_path, 'a') as file:\n",
    "        for record in embeddings:\n",
    "            file.write(json.dumps(record) + '\\n')\n",
    "\n",
    "async def create_embeddings_in_batches(chunks: List[dict], batch_size: int, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Create embeddings for chunks in batches and write to a file.\n",
    "\n",
    "    :param chunks: A list of chunks to embed\n",
    "    :param batch_size: The number of chunks to process in each batch\n",
    "    :param file_path: The file to save embeddings to\n",
    "    \"\"\"\n",
    "    total_chunks = len(chunks)\n",
    "    for start in range(0, total_chunks, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = chunks[start:end]\n",
    "        remaining = total_chunks - end\n",
    "        logging.info(f\"Processing batch {start // batch_size + 1} ({start} to {end - 1})... \"\n",
    "                     f\"{remaining} chunks remaining out of {total_chunks}.\")\n",
    "        await process_batch(batch, file_path)\n",
    "\n",
    "\n",
    "await create_embeddings_in_batches(chunks, batch_size=100, file_path=\"embeddings.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsert Embeddings to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing Pinecone client.\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/jamievoynow/Desktop/code/chat-with-jfk-files/.venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/jamievoynow/Desktop/code/chat-with-jfk-files/.venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:root:Starting upsert of 397 embeddings with batch size 100.\n",
      "INFO:root:Upserting batch 1 with 100 embeddings (indices 0 to 99).\n",
      "INFO:root:Upserting batch 2 with 100 embeddings (indices 100 to 199).\n",
      "INFO:root:Upserting batch 3 with 100 embeddings (indices 200 to 299).\n",
      "INFO:root:Upserting batch 4 with 97 embeddings (indices 300 to 396).\n",
      "INFO:root:Upsert complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'epstein': {'vector_count': 580},\n",
       "                'jfk-docs': {'vector_count': 28386},\n",
       "                'jfk-docs-2025': {'vector_count': 9200}},\n",
       " 'total_vector_count': 38166}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List\n",
    "from pinecone import Pinecone\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def upsert_to_pinecone(\n",
    "    embeddings: List[dict], \n",
    "    upsert_batch_size: int = 100,\n",
    "    namespace: str = \"jfk-docs-2025-master\"\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Upsert embeddings to Pinecone in batches with logging.\n",
    "    \n",
    "    :param embeddings: List of embedding dictionaries.\n",
    "    :param upsert_batch_size: Number of embeddings per upsert batch.\n",
    "    :param namespace: Pinecone namespace.\n",
    "    :return: Pinecone index object.\n",
    "    \"\"\"\n",
    "    logging.info(\"Initializing Pinecone client.\")\n",
    "    pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "    index = pc.Index(\"chat-with-jfk-files\")\n",
    "    \n",
    "    total_embeddings = len(embeddings)\n",
    "    logging.info(f\"Starting upsert of {total_embeddings} embeddings with batch size {upsert_batch_size}.\")\n",
    "    \n",
    "    for batch_start in range(0, total_embeddings, upsert_batch_size):\n",
    "        batch = embeddings[batch_start : batch_start + upsert_batch_size]\n",
    "        vectors = [\n",
    "            {\n",
    "                \"id\": f\"chunk-{str(batch_start + j).zfill(6)}\",\n",
    "                \"values\": emb['embedding'],\n",
    "                \"metadata\": {\"text\": emb['chunk'], \"path\": emb['path']}\n",
    "            }\n",
    "            for j, emb in enumerate(batch)\n",
    "        ]\n",
    "        batch_num = batch_start // upsert_batch_size + 1\n",
    "        logging.info(f\"Upserting batch {batch_num} \"\n",
    "                     f\"with {len(batch)} embeddings (indices {batch_start} to {batch_start + len(batch) - 1}).\")\n",
    "        index.upsert(vectors=vectors, namespace=namespace)\n",
    "    \n",
    "    logging.info(\"Upsert complete.\")\n",
    "    return index\n",
    "\n",
    "with open(\"embeddings.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    embeddings = [json.loads(line) for line in f]\n",
    "\n",
    "index = upsert_to_pinecone(embeddings)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
